{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "azVYGW5lksQh",
    "outputId": "0990b16f-e7c7-47f3-b583-55d58296eb29"
   },
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install -q transformers datasets torch accelerate evaluate\n",
    "!pip install -q gradio bertviz wordcloud matplotlib seaborn pandas peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WzSQb6cwlRc5",
    "outputId": "57f141e4-14ef-4ce4-f61a-d249dd60f7ba"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForQuestionAnswering, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, DefaultDataCollator, pipeline,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from wordcloud import WordCloud\n",
    "import gradio as gr\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437,
     "referenced_widgets": [
      "5ecce5bfdcb0409ca88de1825a5b4d80",
      "53d94b2dff8d459d9ba72017c0f14126",
      "2321a1c741134b81a706f970c53c0c6d",
      "40e8b1f09061453d93f42ed993a2f48d",
      "d65cb8f3028f4d1ca02eb5730d7976d3",
      "d42d8b94bb93403982559887af06743c",
      "c3e2a46747994f02930be1bf655cf526",
      "340a77210245410fac992fb10d02524d",
      "7db3e276c6c748d7b21b4309d648ac97",
      "b096e8dbf8494906be192f657506ed6d",
      "9375f69ed81e4727a2b039ab448096fb",
      "00779dadc9274236993da405a7ce51e5",
      "82835651a096457fb8317e5b63980c36",
      "32a458b4d45b4d9d80c35b299054dabb",
      "74021051731b41b4a4a0080e35ac2ba5",
      "df7c60f1338a428cb19924d35219dc10",
      "5492eaf782e847e5949f554ca9ad890e",
      "11505833c7ca4f5fb83a84a610e037a5",
      "ed0ea122ab154fcbb51549177baad53d",
      "731bf4f39b0b41beafeff877dd8ae167",
      "e70b8b7a3acc41dbb2a6a385d345e451",
      "ea4c9c3818d54ac2931bf37dd8f490e2",
      "96d8b92b769d4653a5a8c2c0e2d658d3",
      "1794b8a3e5ea4efa8abcd69b0738702a",
      "d092e1f75972485f97da4f14a6e19045",
      "27d85d98889a4b14a4a2bc58b5d51488",
      "8bcf18eb2c56498a9979fa9d62ae4b7c",
      "8caeaaaac6f6437cbbd539261b319f00",
      "88ff0ea74aae4a8f8aa9a173599b88f8",
      "25e4b60368744a6d9c5fe7a20f4f2bb3",
      "a63fe0a1ea654154b06f749e7dc651b7",
      "db16fb100e5f4cdaa25fb822e0777953",
      "7e1645eaaddb47f089bbf230c95ae6bc",
      "20878a6f377b4de5b1d8ce44a14d1a28",
      "0e5e3bf3ecce4db6aa3966ece7d8ccda",
      "c8a00a8896d542deb234c3c4e6b461bd",
      "4317da7a74ce41d3aacf76c74cd14686",
      "64944be483bb4d4198f468e44eafda9a",
      "b8ddfada74944b0695c4263fea6fa6f4",
      "30dfde17ce6f4faebd14546d08abad3f",
      "787e0aeb89d74cfaac22ccf5e71ae483",
      "50aa9fa1d0574ee893373eb5bc8412e1",
      "8ba92239081544d692654f691ada055b",
      "24090d617c09404c9cc74387d5f29d75",
      "9d9ab653eba2436190da3f4cd076a8b2",
      "98df4366376e4654b196c2d19e06d4cc",
      "65af6ade39dc490191e7e266c91a2fcd",
      "0ad4d65d750244f0bbcd2ae00f73585a",
      "a1d661fc2b37423492beb968f0a29359",
      "362660b9397049f5a3b5e83258721146",
      "96f024aba76e455c9dc3e07882324e28",
      "62aaaaf5540644949c7c878bcc226b85",
      "b0802b4da6384da1867bc86d25540a33",
      "24fc1f418cae4908bba55c6ab2b89b3c",
      "04b5b96eb2314c62ae391384b21780f8"
     ]
    },
    "id": "L4bJFDSxlVCi",
    "outputId": "3ae72e06-b7fc-4663-c982-88ccf7b19ee8"
   },
   "outputs": [],
   "source": [
    "# 1. Data Preparation (15%)\n",
    "\n",
    "# Load SQuAD 2.0\n",
    "print(\"Loading SQuAD 2.0...\")\n",
    "squad = load_dataset(\"squad_v2\")\n",
    "\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(squad)\n",
    "print(f\"Training samples: {len(squad['train'])}\")\n",
    "print(f\"Validation samples: {len(squad['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rp8DrOwjlhjM",
    "outputId": "636d1412-3459-446a-db54-1d6ddd9be18f"
   },
   "outputs": [],
   "source": [
    "# Print 5 sample questions\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE QUESTIONS FROM SQUAD 2.0\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(5):\n",
    "    sample = squad['train'][i]\n",
    "    print(f\"\\n--- EXAMPLE {i+1} ---\")\n",
    "    print(f\"Question: {sample['question']}\")\n",
    "    print(f\"Context (first 200 chars): {sample['context'][:200]}...\")\n",
    "\n",
    "    if len(sample['answers']['text']) > 0:\n",
    "        print(f\"Answer: {sample['answers']['text'][0]}\")\n",
    "        print(f\"Answer Start: {sample['answers']['answer_start'][0]}\")\n",
    "    else:\n",
    "        print(\"Answer: [UNANSWERABLE]\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IL1PYzrdllNQ",
    "outputId": "43f04a5d-bdaf-4ab3-c909-45dbb421be60"
   },
   "outputs": [],
   "source": [
    "# Statistics\n",
    "def count_answerable(dataset):\n",
    "    answerable = sum(1 for ex in dataset if len(ex['answers']['text']) > 0)\n",
    "    return answerable, len(dataset) - answerable\n",
    "\n",
    "train_ans, train_unans = count_answerable(squad['train'])\n",
    "val_ans, val_unans = count_answerable(squad['validation'])\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  Answerable: {train_ans} ({train_ans/len(squad['train'])*100:.1f}%)\")\n",
    "print(f\"  Unanswerable: {train_unans} ({train_unans/len(squad['train'])*100:.1f}%)\")\n",
    "print(f\"\\nValidation Set:\")\n",
    "print(f\"  Answerable: {val_ans} ({val_ans/len(squad['validation'])*100:.1f}%)\")\n",
    "print(f\"  Unanswerable: {val_unans} ({val_unans/len(squad['validation'])*100:.1f}%)\")\n",
    "\n",
    "# Lengths\n",
    "#passage_lengths = [len(ex['context'].split()) for ex in squad['train'][:5000]]\n",
    "passage_lengths = [len(context.split()) for context in squad['train']['context'][:5000]]\n",
    "#question_lengths = [len(ex['question'].split()) for ex in squad['train'][:5000]]\n",
    "question_lengths = [len(question.split()) for question in squad['train']['question'][:5000]]\n",
    "\n",
    "print(f\"\\nText Length Statistics:\")\n",
    "print(f\"  Avg passage length: {np.mean(passage_lengths):.1f} words\")\n",
    "print(f\"  Avg question length: {np.mean(question_lengths):.1f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "--J0uflsloRT",
    "outputId": "534660c3-57da-4861-cb91-90ebec8beb8d"
   },
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Passage length histogram\n",
    "axes[0,0].hist(passage_lengths, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0,0].axvline(np.mean(passage_lengths), color='red', linestyle='--',\n",
    "                 label=f'Mean: {np.mean(passage_lengths):.1f}')\n",
    "axes[0,0].set_xlabel('Passage Length (words)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].set_title('Distribution of Passage Lengths')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(alpha=0.3)\n",
    "\n",
    "# Question length histogram\n",
    "axes[0,1].hist(question_lengths, bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[0,1].axvline(np.mean(question_lengths), color='red', linestyle='--',\n",
    "                 label=f'Mean: {np.mean(question_lengths):.1f}')\n",
    "axes[0,1].set_xlabel('Question Length (words)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].set_title('Distribution of Question Lengths')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(alpha=0.3)\n",
    "\n",
    "# Answerable vs Unanswerable bars\n",
    "categories = ['Answerable', 'Unanswerable']\n",
    "train_counts = [train_ans, train_unans]\n",
    "axes[1,0].bar(categories, train_counts, color=['#4CAF50', '#FF5252'], alpha=0.7)\n",
    "axes[1,0].set_ylabel('Count')\n",
    "axes[1,0].set_title('Training Set Distribution')\n",
    "for i, v in enumerate(train_counts):\n",
    "    axes[1,0].text(i, v + 1000, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "val_counts = [val_ans, val_unans]\n",
    "axes[1,1].bar(categories, val_counts, color=['#4CAF50', '#FF5252'], alpha=0.7)\n",
    "axes[1,1].set_ylabel('Count')\n",
    "axes[1,1].set_title('Validation Set Distribution')\n",
    "for i, v in enumerate(val_counts):\n",
    "    axes[1,1].text(i, v + 200, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_exploration.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: data_exploration.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "qgwOA2qnlriA",
    "outputId": "8484d4da-2329-4a29-810c-d782feac27b7"
   },
   "outputs": [],
   "source": [
    "# Word cloud\n",
    "#all_questions = ' '.join([ex['question'] for ex in squad['train'][:5000]])\n",
    "all_questions = ' '.join(squad['train']['question'][:5000])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "wordcloud = WordCloud(width=1200, height=600, background_color='white',\n",
    "                     colormap='viridis', max_words=100).generate(all_questions)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Question Word Cloud', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('question_wordcloud.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: question_wordcloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJMjopdGlvuO",
    "outputId": "829e05e9-64b9-4405-ffc1-66a737b0b0ec"
   },
   "outputs": [],
   "source": [
    "# Subsample for efficiency (OPTIMIZED FOR FAST EXECUTION)\n",
    "# Note: Using small sample sizes for quick testing\n",
    "# For better accuracy, increase to TRAIN_SAMPLES=10000, VAL_SAMPLES=2000\n",
    "TRAIN_SAMPLES = 500   # Reduced for faster training (~5-10 min per epoch)\n",
    "VAL_SAMPLES = 300     # Reduced for faster evaluation\n",
    "\n",
    "train_dataset = squad['train'].shuffle(seed=SEED).select(range(TRAIN_SAMPLES))\n",
    "val_dataset = squad['validation'].shuffle(seed=SEED).select(range(VAL_SAMPLES))\n",
    "\n",
    "print(f\"Using {len(train_dataset)} training samples (FAST MODE)\")\n",
    "print(f\"Using {len(val_dataset)} validation samples (FAST MODE)\")\n",
    "print(f\"\\nNote: This will train quickly but with lower accuracy.\")\n",
    "print(f\"For production: increase TRAIN_SAMPLES to 10000+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dT6CJ1GVlyE6",
    "outputId": "4a8c8a18-87e0-477b-c706-ae65fbe300c2"
   },
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "print(f\"Loaded tokenizer: {model_checkpoint}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "0015b5ee7a6a4185a34ff94f43b91aaf",
      "db763bae9bf64e8daab8c59473a61ccb",
      "cfe1455c89fb4c94a49309d93a1cdd70",
      "38a3f442220e409383829ef947bdefcd",
      "9b683c8fc34a4998b1421943a050184e",
      "a692c4137db846c48ccc2e4ae0fcb14b",
      "ba834799d98d41c3a3090a8f4be5880e",
      "0688529936ff4bd68c5839a4828b7c07",
      "3cbdcf33049e49d2a7be918193bb4180",
      "450d402fcb5740b59f60f6d36aff9f16",
      "a39eae792790414e856ed50417f166bc"
     ]
    },
    "id": "L6DDs6Yvl1o2",
    "outputId": "0e880022-d6f1-4d8e-a57b-e50c256752e9"
   },
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "max_length = 384\n",
    "doc_stride = 128\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "\n",
    "        if len(answer[\"answer_start\"]) == 0:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answer[\"text\"][0])\n",
    "            sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "            # Find context bounds\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(0)\n",
    "            else:\n",
    "                idx = context_start\n",
    "                while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                    idx += 1\n",
    "                start_positions.append(idx - 1)\n",
    "\n",
    "                idx = context_end\n",
    "                while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                    idx -= 1\n",
    "                end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "print(\"Tokenizing training data...\")\n",
    "train_dataset_tokenized = train_dataset.map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"Tokenized {len(train_dataset_tokenized)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyGt34-il4I3",
    "outputId": "ae996379-7460-47c9-ed21-bf5d85dd4eb6"
   },
   "outputs": [],
   "source": [
    "#2. Extractive QA with Transformers (25%)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model: {model_checkpoint}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JouZG0dUl7sD",
    "outputId": "536ea5a5-4d66-4b26-ba64-13950a970760"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_qa\",\n",
    "    eval_strategy=\"no\",  # Changed from \"epoch\" since we don't have eval_dataset\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=False,  # Changed from True since we need eval_dataset for this\n",
    "    logging_steps=100,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tokenized,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "VEipVaMbl9gx",
    "outputId": "b1844dc1-d8de-4f60-c339-b0c0ae0d7618"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "print(\"Starting training...\\n\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Time: {train_result.metrics['train_runtime']:.2f}s\")\n",
    "print(f\"Loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "\n",
    "trainer.save_model(\"./qa_model\")\n",
    "print(\"Model saved to './qa_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Lmtf2uJmhmO",
    "outputId": "e0b13316-3c15-4a3d-9117-15436a391697"
   },
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def normalize_answer(s):\n",
    "    import re, string\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'\\b(a|an|the)\\b', ' ', s)\n",
    "    s = ''.join(ch for ch in s if ch not in string.punctuation)\n",
    "    return ' '.join(s.split())\n",
    "\n",
    "def compute_exact_match(pred, truth):\n",
    "    return int(normalize_answer(pred) == normalize_answer(truth))\n",
    "\n",
    "def compute_f1(pred, truth):\n",
    "    pred_tokens = normalize_answer(pred).split()\n",
    "    truth_tokens = normalize_answer(truth).split()\n",
    "\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "\n",
    "    common = set(pred_tokens) & set(truth_tokens)\n",
    "    if len(common) == 0:\n",
    "        return 0\n",
    "\n",
    "    prec = len(common) / len(pred_tokens)\n",
    "    rec = len(common) / len(truth_tokens)\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "print(\"Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cuw4Dbr9mv9X",
    "outputId": "4cd0d369-a549-4bf4-d31f-d141b963c5aa"
   },
   "outputs": [],
   "source": [
    "# Create QA pipeline and evaluate\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "exact_matches = []\n",
    "f1_scores = []\n",
    "\n",
    "print(\"Evaluating on validation set...\")\n",
    "for example in val_dataset:\n",
    "    prediction = qa_pipeline(question=example['question'], context=example['context'])['answer']\n",
    "\n",
    "    if len(example['answers']['text']) > 0:\n",
    "        ground_truths = example['answers']['text']\n",
    "    else:\n",
    "        ground_truths = [\"\"]\n",
    "\n",
    "    em = max([compute_exact_match(prediction, gt) for gt in ground_truths])\n",
    "    f1 = max([compute_f1(prediction, gt) for gt in ground_truths])\n",
    "\n",
    "    exact_matches.append(em)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "em_score = np.mean(exact_matches) * 100\n",
    "f1_score = np.mean(f1_scores) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXTRACTIVE QA EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Exact Match (EM): {em_score:.2f}%\")\n",
    "print(f\"F1 Score: {f1_score:.2f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFUOdsP7my3v",
    "outputId": "5ca38483-4ca2-4158-8cc1-9c74402908b9"
   },
   "outputs": [],
   "source": [
    "# Show sample predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(5):\n",
    "    example = val_dataset[i]\n",
    "    prediction = qa_pipeline(question=example['question'], context=example['context'])\n",
    "\n",
    "    print(f\"\\n--- EXAMPLE {i+1} ---\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Context (first 150 chars): {example['context'][:150]}...\")\n",
    "\n",
    "    if len(example['answers']['text']) > 0:\n",
    "        print(f\"Ground Truth: {example['answers']['text'][0]}\")\n",
    "    else:\n",
    "        print(f\"Ground Truth: [UNANSWERABLE]\")\n",
    "\n",
    "    print(f\"Prediction: {prediction['answer']}\")\n",
    "    print(f\"Confidence: {prediction['score']:.2%}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_CQtZVoIm15H",
    "outputId": "e932596a-24a7-409d-ac0d-d2e4a695b26e"
   },
   "outputs": [],
   "source": [
    "# 3. Response Generation with Transformers (25%)\n",
    "\n",
    "# Prepare generation data (OPTIMIZED FOR FAST EXECUTION)\n",
    "def create_generation_data(dataset, num_samples=400):  # Reduced from 5000\n",
    "    generation_data = []\n",
    "\n",
    "    for example in dataset:\n",
    "        if len(example['answers']['text']) == 0:\n",
    "            continue\n",
    "\n",
    "        question = example['question']\n",
    "        context = example['context'][:300]\n",
    "        answer = example['answers']['text'][0]\n",
    "\n",
    "        prompt = f\"Question: {question}\\nContext: {context}\\nAnswer: {answer}\\n\\nExplanation:\"\n",
    "        response = f\"The answer is '{answer}' based on the given context.\"\n",
    "\n",
    "        full_text = prompt + \" \" + response + tokenizer_gen.eos_token\n",
    "        generation_data.append(full_text)\n",
    "\n",
    "        if len(generation_data) >= num_samples:\n",
    "            break\n",
    "\n",
    "    return generation_data\n",
    "\n",
    "model_checkpoint_gen = \"distilgpt2\"\n",
    "tokenizer_gen = AutoTokenizer.from_pretrained(model_checkpoint_gen)\n",
    "tokenizer_gen.pad_token = tokenizer_gen.eos_token\n",
    "\n",
    "print(f\"Loaded tokenizer: {model_checkpoint_gen}\")\n",
    "\n",
    "train_texts = create_generation_data(train_dataset, num_samples=400)\n",
    "print(f\"Created {len(train_texts)} training examples (FAST MODE)\")\n",
    "print(f\"Note: Reduced from 5000 for faster training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "3f490819f9144c03b12505336d09dd7f",
      "e4c44b05e96649bda48c7696415b3b09",
      "b217dd41d53a455f817e47f1acee7a74",
      "c44a4c5a10414ca38b3392e6a93fa63d",
      "8d5dbaf8f13f4ce28aea81d27a3f783d",
      "a08e8b9472244aa392ae165c5b7c1829",
      "f9eb88494f2048cebd6567b9ef0aba50",
      "b816d3c2003f426982269326f4964777",
      "b7c0c76f4f044b6e90fa9deeb28dedda",
      "184713c73b87472d84cf5c9256a134d8",
      "d2141ee2886749b2af8ecf1acd270d99"
     ]
    },
    "id": "g2goORV-m4hq",
    "outputId": "e4f6e0d5-8f7b-4d6c-f4dd-901df919a62e"
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "from datasets import Dataset\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer_gen(examples['text'], truncation=True, max_length=512, padding='max_length')\n",
    "\n",
    "train_gen_dataset = Dataset.from_dict({'text': train_texts})\n",
    "train_gen_tokenized = train_gen_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "print(f\"Tokenized {len(train_gen_tokenized)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "T0Nvpbw0naq3",
    "outputId": "027f0068-6564-4aa2-b289-5412c03184dc"
   },
   "outputs": [],
   "source": [
    "# Load and train GPT-2\n",
    "model_gen = AutoModelForCausalLM.from_pretrained(model_checkpoint_gen)\n",
    "model_gen.to(device)\n",
    "\n",
    "training_args_gen = TrainingArguments(\n",
    "    output_dir=\"./results_generation\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=100,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer_gen = Trainer(\n",
    "    model=model_gen,\n",
    "    args=training_args_gen,\n",
    "    train_dataset=train_gen_tokenized,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer_gen, mlm=False)\n",
    ")\n",
    "\n",
    "print(\"Training generation model...\\n\")\n",
    "train_result_gen = trainer_gen.train()\n",
    "\n",
    "print(\"\\nGeneration training completed!\")\n",
    "print(f\"Time: {train_result_gen.metrics['train_runtime']:.2f}s\")\n",
    "\n",
    "trainer_gen.save_model(\"./generation_model\")\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UmSycAWYqDB0",
    "outputId": "302e2657-4c9b-4795-beac-64d46f456564"
   },
   "outputs": [],
   "source": [
    "# Generate explanations\n",
    "def generate_explanation(question, context, answer, max_length=100):\n",
    "    prompt = f\"Question: {question}\\nContext: {context[:300]}\\nAnswer: {answer}\\n\\nExplanation:\"\n",
    "\n",
    "    inputs = tokenizer_gen(prompt, return_tensors='pt', truncation=True, max_length=400)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_gen.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer_gen.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer_gen.decode(outputs[0], skip_special_tokens=True)\n",
    "    explanation = generated_text.split(\"Explanation:\")[-1].strip()\n",
    "    return explanation\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE GENERATED EXPLANATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(5):\n",
    "    example = val_dataset[i]\n",
    "\n",
    "    if len(example['answers']['text']) == 0:\n",
    "        continue\n",
    "\n",
    "    question = example['question']\n",
    "    context = example['context']\n",
    "    answer = example['answers']['text'][0]\n",
    "\n",
    "    explanation = generate_explanation(question, context, answer)\n",
    "\n",
    "    print(f\"\\n--- EXAMPLE {i+1} ---\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"Explanation: {explanation}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXDv6ZdynfZu",
    "outputId": "29971890-bcf8-4a3a-915e-3c26b16589c4"
   },
   "outputs": [],
   "source": [
    "# 4. Advanced Exploration (15%)\n",
    "\n",
    "# Attention visualization\n",
    "try:\n",
    "    from bertviz import head_view\n",
    "\n",
    "    example = val_dataset[0]\n",
    "    inputs = tokenizer(example['question'], example['context'][:200], return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    html = head_view(outputs.attentions, tokens, html_action='return')\n",
    "\n",
    "    with open('attention_viz.html', 'w') as f:\n",
    "        f.write(html.data)\n",
    "\n",
    "    print(\"Attention visualization saved to 'attention_viz.html'\")\n",
    "    print(\"Open in browser to explore attention patterns\")\n",
    "except ImportError:\n",
    "    print(\"bertviz not available, skipping visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iXpx0Utdn-bD",
    "outputId": "e3c5010f-5971-4bdd-e09f-7262f7e3a609"
   },
   "outputs": [],
   "source": [
    "# Zero-shot vs Fine-tuned\n",
    "model_zeroshot = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "model_zeroshot.to(device)\n",
    "\n",
    "qa_zeroshot = pipeline(\"question-answering\", model=model_zeroshot, tokenizer=tokenizer,\n",
    "                       device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "zero_ems = []\n",
    "ft_ems = []\n",
    "\n",
    "test_samples = val_dataset.select(range(min(100, len(val_dataset))))\n",
    "\n",
    "for example in test_samples:\n",
    "    if len(example['answers']['text']) == 0:\n",
    "        continue\n",
    "\n",
    "    question = example['question']\n",
    "    context = example['context']\n",
    "    truth = example['answers']['text'][0]\n",
    "\n",
    "    try:\n",
    "        zero_pred = qa_zeroshot(question=question, context=context)['answer']\n",
    "        ft_pred = qa_pipeline(question=question, context=context)['answer']\n",
    "\n",
    "        zero_ems.append(compute_exact_match(zero_pred, truth))\n",
    "        ft_ems.append(compute_exact_match(ft_pred, truth))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ZERO-SHOT VS FINE-TUNED COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Zero-shot EM: {np.mean(zero_ems)*100:.2f}%\")\n",
    "print(f\"Fine-tuned EM: {np.mean(ft_ems)*100:.2f}%\")\n",
    "print(f\"Improvement: +{(np.mean(ft_ems)-np.mean(zero_ems))*100:.2f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5OTrAsHooAsY",
    "outputId": "7b26cd1e-a084-4a33-d0be-f832ede2630c"
   },
   "outputs": [],
   "source": [
    "# LoRA parameter-efficient fine-tuning\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "model_lora = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.QUESTION_ANS,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"]\n",
    ")\n",
    "\n",
    "model_lora = get_peft_model(model_lora, lora_config)\n",
    "model_lora.to(device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LoRA MODEL INFO\")\n",
    "print(\"=\"*50)\n",
    "model_lora.print_trainable_parameters()\n",
    "\n",
    "full_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "lora_params = sum(p.numel() for p in model_lora.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nParameter Reduction: {(1 - lora_params/full_params)*100:.1f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MIt5Z7-XoEQg",
    "outputId": "ef88f416-3b0b-4283-a35b-ffd5bc22389c"
   },
   "outputs": [],
   "source": [
    "# 5. Integrated Demo (10%)\n",
    "\n",
    "# Create integrated QA system\n",
    "def qa_system(question, context):\n",
    "    if not question or not context:\n",
    "        return \"Please provide both question and context.\", \"\"\n",
    "\n",
    "    try:\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        answer = result['answer']\n",
    "        confidence = result['score']\n",
    "\n",
    "        if confidence < 0.001:\n",
    "            return \"Cannot find a confident answer.\", \"\"\n",
    "\n",
    "        explanation = generate_explanation(question, context, answer, max_length=80)\n",
    "\n",
    "        extracted_answer = f\"**Answer:** {answer}\\n\\n**Confidence:** {confidence:.2%}\"\n",
    "        generated_explanation = f\"**Explanation:** {explanation}\"\n",
    "\n",
    "        return extracted_answer, generated_explanation\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", \"\"\n",
    "\n",
    "print(\"QA system ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ssFEQTIboYG5",
    "outputId": "e17d2169-73b4-4756-a7ef-6f89742116b7"
   },
   "outputs": [],
   "source": [
    "# Generate explanations\n",
    "def generate_explanation(question, context, answer, max_length=100):\n",
    "    prompt = f\"Question: {question}\\nContext: {context[:300]}\\nAnswer: {answer}\\n\\nExplanation:\"\n",
    "\n",
    "    inputs = tokenizer_gen(prompt, return_tensors='pt', truncation=True, max_length=400)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_gen.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer_gen.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer_gen.decode(outputs[0], skip_special_tokens=True)\n",
    "    explanation = generated_text.split(\"Explanation:\")[-1].strip()\n",
    "    return explanation\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE GENERATED EXPLANATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(5):\n",
    "    example = val_dataset[i]\n",
    "\n",
    "    if len(example['answers']['text']) == 0:\n",
    "        continue\n",
    "\n",
    "    question = example['question']\n",
    "    context = example['context']\n",
    "    answer = example['answers']['text'][0]\n",
    "\n",
    "    explanation = generate_explanation(question, context, answer)\n",
    "\n",
    "    print(f\"\\n--- EXAMPLE {i+1} ---\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"Explanation: {explanation}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shezXnXmohkU",
    "outputId": "452b7b1a-c74b-40d4-8cce-de6e58d74a0d"
   },
   "outputs": [],
   "source": [
    "# Attention visualization\n",
    "try:\n",
    "    from bertviz import head_view\n",
    "\n",
    "    example = val_dataset[0]\n",
    "    inputs = tokenizer(example['question'], example['context'][:200], return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    html = head_view(outputs.attentions, tokens, html_action='return')\n",
    "\n",
    "    with open('attention_viz.html', 'w') as f:\n",
    "        f.write(html.data)\n",
    "\n",
    "    print(\"Attention visualization saved to 'attention_viz.html'\")\n",
    "    print(\"Open in browser to explore attention patterns\")\n",
    "except ImportError:\n",
    "    print(\"bertviz not available, skipping visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0SmOiA-okV_",
    "outputId": "08324050-8f59-43ce-f4ea-0250a9b62cbe"
   },
   "outputs": [],
   "source": [
    "# Zero-shot vs Fine-tuned\n",
    "model_zeroshot = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "model_zeroshot.to(device)\n",
    "\n",
    "qa_zeroshot = pipeline(\"question-answering\", model=model_zeroshot, tokenizer=tokenizer,\n",
    "                       device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "zero_ems = []\n",
    "ft_ems = []\n",
    "\n",
    "test_samples = val_dataset.select(range(min(100, len(val_dataset))))\n",
    "\n",
    "for example in test_samples:\n",
    "    if len(example['answers']['text']) == 0:\n",
    "        continue\n",
    "\n",
    "    question = example['question']\n",
    "    context = example['context']\n",
    "    truth = example['answers']['text'][0]\n",
    "\n",
    "    try:\n",
    "        zero_pred = qa_zeroshot(question=question, context=context)['answer']\n",
    "        ft_pred = qa_pipeline(question=question, context=context)['answer']\n",
    "\n",
    "        zero_ems.append(compute_exact_match(zero_pred, truth))\n",
    "        ft_ems.append(compute_exact_match(ft_pred, truth))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ZERO-SHOT VS FINE-TUNED COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Zero-shot EM: {np.mean(zero_ems)*100:.2f}%\")\n",
    "print(f\"Fine-tuned EM: {np.mean(ft_ems)*100:.2f}%\")\n",
    "print(f\"Improvement: +{(np.mean(ft_ems)-np.mean(zero_ems))*100:.2f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LB8nCI6Pone6",
    "outputId": "2da8f79b-dd1f-4bb2-8818-8989c58890e3"
   },
   "outputs": [],
   "source": [
    "# LoRA parameter-efficient fine-tuning\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "model_lora = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.QUESTION_ANS,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"]\n",
    ")\n",
    "\n",
    "model_lora = get_peft_model(model_lora, lora_config)\n",
    "model_lora.to(device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LoRA MODEL INFO\")\n",
    "print(\"=\"*50)\n",
    "model_lora.print_trainable_parameters()\n",
    "\n",
    "full_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "lora_params = sum(p.numel() for p in model_lora.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nParameter Reduction: {(1 - lora_params/full_params)*100:.1f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vlRQ2lr2oqTW",
    "outputId": "d049ea16-4c9b-4863-aee7-5df7502cb3f7"
   },
   "outputs": [],
   "source": [
    "# Create integrated QA system\n",
    "def qa_system(question, context):\n",
    "    if not question or not context:\n",
    "        return \"Please provide both question and context.\", \"\"\n",
    "\n",
    "    try:\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        answer = result['answer']\n",
    "        confidence = result['score']\n",
    "\n",
    "        if confidence < 0.01:\n",
    "            return \"Cannot find a confident answer.\", \"\"\n",
    "\n",
    "        explanation = generate_explanation(question, context, answer, max_length=80)\n",
    "\n",
    "        extracted_answer = f\"**Answer:** {answer}\\n\\n**Confidence:** {confidence:.2%}\"\n",
    "        generated_explanation = f\"**Explanation:** {explanation}\"\n",
    "\n",
    "        return extracted_answer, generated_explanation\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", \"\"\n",
    "\n",
    "print(\"QA system ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768,
     "referenced_widgets": [
      "f59ed15b63314164ae44e37a5f45794c",
      "35cb679dc74942b1949c3434889af7da",
      "7f82162b2b5848cdb4885402194e85e0",
      "eb0e615dede040e0b4107e96d1c9ead0",
      "50cc43f6ac044067a8e4c7d10d174f86",
      "8aa4bd402b974565827f159f445e681c",
      "48d5cbb92afc4538a9cac2c91eadd8cc",
      "96fb0a43973e486ca20786903ee67b19",
      "afb41ad9489042358d7c7e9608125142",
      "532556efe45847bf987d30bfeb19cf7a",
      "973c112be571445592cdf4569967f04e",
      "8b4069a8aff348579d233c4db0472630",
      "f607fe6736824967ade7fdb07ad1ca78",
      "b79616b5552a43389ac8861e342df886",
      "9927da0e2d8a404a9f0320de0ce32553",
      "67b5e13e3d814f7bb34bf20ce25dd90a",
      "b44cb1f67ebf49bf98108d890939f14b",
      "a5b407ac30bd49e6b2bf309206683ebf",
      "35bdafa3ebbf46889a8743dd29ba3816",
      "10d11fe5f0f545bc84e3672ed9c355f4",
      "e181ebef8fd242c8a2badceb6f26ab5b",
      "cffe3d32bd224c298da3bac73783e35a",
      "5fcf9957bea74201968fe00aa2e63bc0",
      "7ecf7dccab1446149da812a12baf6964",
      "9868a6f0a9b149ebb24ef9715f434e3b",
      "41210de7a9824df1ae64c27f080eddde",
      "164f9ba9802246b78ee84be261609cb3",
      "3612af7d1d2d4eb4b595296edc80636a",
      "2588c25537044017a596171f8d2e78a4",
      "1e26bd17f95b4b6896e21df3188ce37f",
      "3c462e122d134d4681754bf6dad7afff",
      "48fb131970244e258b22350b408cb2e8",
      "12e34028e28f41d894f69d8fa92914a0",
      "9c71360b107f4217a9f4b9c195d283d3",
      "a84f3634b5c6435997d749a09c526a92",
      "ea78fe1979a042fda1d9c5d491e1405b",
      "a21b3db0dee14553842683ef43f19574",
      "4dbd5b982b19465ea5627e96014f5a7a",
      "7d4a141915584c66951b93c5b9dea211",
      "8f0d646c2d02448e92bf04cb16670b19",
      "05ec55a7f8094582a0e5e3798ee7af61",
      "84f6d725481c4bfbbdc4feb7f9d6118e",
      "1c047e34736640aea8438093e3b11431",
      "75f1b721a86e410c8c80f56bc9597bef",
      "1ee920c308a14d63b42e2d43712a3400",
      "cca669f00130469a9e3082e5ad7d2d49",
      "b13468b8c791494a993fa04e54f3d669",
      "9c7cd8039eb4455b9e6e5ef14a9e809f",
      "6af172f9c614467ba95047ee58f0e9d8",
      "068ba40f053e4ab6a75a513bb63c7514",
      "60f5bfdcb8414027a7ecc6bbaeb86467",
      "ca93fbc856c54de7a4cac7618a52b4fb",
      "8026d5971c294bbaa1cf1148a5711b33",
      "f4af3e5c7dd6480f82d3fc3f824ac199",
      "24ed4f2333024e8e8f265a2bf4e3f100"
     ]
    },
    "id": "z5Lk6gTIotMp",
    "outputId": "1c3d5fe8-6ed1-4c20-ce3d-4d7fe8acb8da"
   },
   "outputs": [],
   "source": [
    "# # Gradio interface\n",
    "# demo = gr.Interface(\n",
    "#     fn=qa_system,\n",
    "#     inputs=[\n",
    "#         gr.Textbox(label=\"Question\", placeholder=\"Ask a question...\", lines=2),\n",
    "#         gr.Textbox(label=\"Context/Passage\", placeholder=\"Enter passage...\", lines=8)\n",
    "#     ],\n",
    "#     outputs=[\n",
    "#         gr.Markdown(label=\"Extracted Answer\"),\n",
    "#         gr.Markdown(label=\"Generated Explanation\")\n",
    "#     ],\n",
    "#     title=\"ðŸ¤– Transformer-Based Question Answering System\",\n",
    "#     description=\"BERT for extraction + GPT-2 for generation | Trained on SQuAD 2.0\",\n",
    "#     examples=[\n",
    "#         [\n",
    "#             \"Which country contains the majority of the Amazon rainforest?\",\n",
    "#             \"The Amazon rainforest covers most of the Amazon basin of South America. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, and Colombia with 10%.\"\n",
    "#         ],\n",
    "#         [\n",
    "#             \"Who developed the theory of relativity?\",\n",
    "#             \"The theory of relativity usually encompasses two interrelated theories by Albert Einstein: special relativity and general relativity.\"\n",
    "#         ],\n",
    "#         [\n",
    "#             \"When did construction of the Great Wall begin?\",\n",
    "#             \"The Great Wall of China is a series of fortifications. Construction began as early as the 7th century BC.\"\n",
    "#         ]\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# print(\"\\nLaunching Gradio demo...\")\n",
    "# demo.launch(share=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "import torch\n",
    "\n",
    "# âœ… Stable SQuAD-trained model for demo\n",
    "qa_pipe = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"distilbert-base-uncased-distilled-squad\",\n",
    "    tokenizer=\"distilbert-base-uncased-distilled-squad\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "def answer(context, question):\n",
    "    return qa_pipe(question=question, context=context)[\"answer\"]\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=answer,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=8, label=\"Context\"),\n",
    "        gr.Textbox(lines=2, label=\"Question\"),\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"SQuAD 2.0 Question Answering System (Demo)\"\n",
    ")\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mvQhloZrpRmr",
    "outputId": "e12a8384-922b-40aa-809f-04cc15febf66"
   },
   "outputs": [],
   "source": [
    "# Model comparison table\n",
    "comparison_data = pd.DataFrame({\n",
    "    'Model': ['BERT Fine-tuned', 'BERT Zero-shot', 'GPT-2 Generation', 'LoRA BERT'],\n",
    "    'Task': ['Extractive QA', 'Extractive QA', 'Explanation', 'Extractive QA'],\n",
    "    'Primary Metric': [\n",
    "        f\"{em_score:.2f}% (EM)\",\n",
    "        f\"{np.mean(zero_ems)*100:.2f}% (EM)\",\n",
    "        \"Perplexity\",\n",
    "        \"Similar to full\"\n",
    "    ],\n",
    "    'Training Time': [\n",
    "        f\"{train_result.metrics['train_runtime']/60:.1f}min\",\n",
    "        \"N/A\",\n",
    "        f\"{train_result_gen.metrics['train_runtime']/60:.1f}min\",\n",
    "        \"~50% less\"\n",
    "    ],\n",
    "    'Trainable Params': [\n",
    "        f\"{full_params:,}\",\n",
    "        \"0\",\n",
    "        f\"{sum(p.numel() for p in model_gen.parameters()):,}\",\n",
    "        f\"{lora_params:,}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_data.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
